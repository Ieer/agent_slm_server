# Agent SLM Server (åŸ y26hkx_server) æœ¬åœ° LLM / OpenAI ç›¸å®¹æœå‹™

è¼•é‡ã€å¯æœ¬æ©Ÿé›¢ç·šé‹è¡Œçš„ Qwen 0.5B + å¤š embedding æ¨¡å‹ OpenAI ç›¸å®¹ API æœå‹™ã€‚

æ ¸å¿ƒèƒ½åŠ›ï¼š

- Chat / Completion (`/v1/chat/completions`, `/v1/completions`)
- Embeddings (`/v1/embeddings` è‡ªå‹•æƒæ `models/embedding/*`)
- æ¨¡å‹åˆ—å‡ºèˆ‡é‡æ–°è¼‰å…¥ (`/v1/models`, `/v1/reload-embeddings`)
- å¥åº·æª¢æŸ¥èˆ‡æŒ‡æ¨™ (`/health`, `/metrics`)
- å·¥å…·å‘¼å«è§£æ (å¤šç¨®æ¨™è¨˜æ ¼å¼)
- åŸºç¤ä¸²æµç”Ÿæˆèƒ½åŠ›ï¼ˆå¯æ“´å…… SSEï¼‰
- å¤šå¾Œç«¯ï¼š`transformers` èˆ‡ `llama.cpp` (GGUF) å¯åˆ‡æ›

å®Œæ•´ä½¿ç”¨ã€ç’°å¢ƒè®Šæ•¸ã€æ€§èƒ½èª¿æ ¡ã€æ•…éšœæ’é™¤è«‹è¦‹ï¼š
ğŸ‘‰ `USAGE_GUIDE.md`ï¼ˆé€²éšæŒ‡å—ï¼‰
ğŸ‘‰ `docs/memory-optimization-guide.md`ï¼ˆè¨˜æ†¶é«”/é‡åŒ–ç­–ç•¥ï¼‰

## æ–°å¢ï¼šllama.cpp (GGUF) å¾Œç«¯å¿«é€ŸæŒ‡å—

ç•¶ `transformers` + åŸå§‹ safetensors å…§å­˜ä¸è¶³æˆ–é‡åˆ° Windows 1455ï¼ˆé é¢æª”éå°ï¼‰æ™‚ï¼Œå¯åˆ‡æ›è‡³ GGUF é‡åŒ–æ¨¡å‹ï¼š

```powershell
# å®‰è£ï¼ˆå¯é¸ extrasï¼‰
pip install -e .[llama]
# æˆ–åªè£æ ¸å¿ƒå¥—ä»¶
pip install llama-cpp-python

# å•Ÿå‹•ï¼ˆä½¿ç”¨æä¾›çš„ Q4_K_M GGUF ç¯„ä¾‹ï¼‰
$env:MODEL_BACKEND="llama.cpp"
$env:MODEL_PATH="models/qwen/Qwen2.5-0.5B-Instruct-GGUF/Qwen2.5-0.5B-Instruct-Q4_K_M.gguf"
$env:COMPILE_MODEL="0"
$env:MAX_INPUT_TOKENS="1024"
uvicorn agent_slm_server.slm_server:app --host 127.0.0.1 --port 8001
```

æ›´ç°¡ï¼š

```powershell
powershell -ExecutionPolicy Bypass -File scripts/start_llama_cpp.ps1
```
æˆ–ï¼ˆLinux / macOS / WSLï¼‰ï¼š

```bash
bash scripts/start_llama_cpp.sh
```
èª¿åƒï¼ˆå¯é¸ï¼‰

```powershell
$env:LLAMA_THREADS="6"   # CPU æ ¸å¿ƒæ•¸
$env:LLAMA_CTX="1024"    # ä¸Šä¸‹æ–‡é•·åº¦ (èˆ‡ MAX_INPUT_TOKENS å”èª¿)
```

å›é€€ transformersï¼šæ¸…é™¤/ä¿®æ”¹ `MODEL_BACKEND` æˆ–è¨­ç‚º `transformers`ã€‚

---

## ç›®éŒ„çµæ§‹ï¼ˆæ ¸å¿ƒï¼‰

```text
models/
  qwen/Qwen2.5-0.5B-Instruct/   # ä¸»è¦å°è©±æ¨¡å‹
  embedding/                    # å¤šå€‹ embedding å­è³‡æ–™å¤¾
scripts/                        # å•Ÿå‹•èˆ‡é‹ç¶­è…³æœ¬
  start_server.py               # ä¾è³´/æ¨¡å‹/ç’°å¢ƒæª¢æŸ¥ + å•Ÿå‹•ç°¡åŒ–æœå‹™
  start_optimized.(sh|ps1)      # è¨˜æ†¶é«”åµæ¸¬ + é‡åŒ–/é™åˆ¶è‡ªå‹•è¨­å®š
  start_llama_cpp.(sh|ps1)      # GGUF / llama.cpp å¾Œç«¯å•Ÿå‹•
slm_server.py                   # ä¸»æœå‹™ (è‹¥å¾ŒçºŒé·ç§»å¯æ”¾å…¥ src/)
slm_server_simple.py            # ç°¡åŒ–/ä½ä¾è³´ç‰ˆæœ¬
qwenchat.py                     # QwenChatAPI å°è£
memory_monitor.py               # è¨˜æ†¶é«”ç›£æ§å·¥å…·
performance_config.py           # æ€§èƒ½/è³‡æºé…ç½®
requirements*.txt               # æ­£é€æ­¥è¢« pyproject.extras å–ä»£
pyproject.toml                  # å¥—ä»¶èˆ‡ extras ç®¡ç†
```

 
 
## å®‰è£æ–¹å¼ï¼ˆå»ºè­° Python 3.11ï¼‰

æœ€å°å®‰è£ï¼š

```powershell
python -m venv .venv
. .venv/Scripts/Activate.ps1
pip install --upgrade pip
pip install -e .
```

é–‹ç™¼ç’°å¢ƒï¼ˆlint / type / testï¼‰ï¼š

```powershell
pip install -e .[dev]
```

é‡åŒ–/å„ªåŒ–ï¼š

```powershell
pip install -e .[optimized]
```

åƒ…é‡åŒ–ï¼ˆä¿ç•™ quantï¼‰ï¼š

```powershell
pip install -e .[quant]
```

çµ„åˆå®‰è£ï¼š

```powershell
pip install -e .[optimized,dev]
```

GPU ç‰ˆ Torchï¼ˆCUDA 12.1 ç¯„ä¾‹ï¼‰ï¼š

```powershell
pip install torch --index-url https://download.pytorch.org/whl/cu121
```

> æ›´å¤šä¾è³´èˆ‡å„ªåŒ–èªªæ˜è«‹è¦‹ `USAGE_GUIDE.md`ã€‚

 
 
## å•Ÿå‹•

æœ€ç°¡ï¼ˆå®Œæ•´ä¸»æœå‹™ï¼‰:

```powershell
uvicorn agent_slm_server.slm_server:app --host 127.0.0.1 --port 8001
```

ç°¡åŒ–ç‰ˆï¼ˆä½ä¾è³´ï¼‰:

```powershell
uvicorn agent_slm_server.slm_server_simple:app --host 127.0.0.1 --port 8001
```

å•Ÿå‹•è…³æœ¬ï¼ˆä¾è³´/æ¨¡å‹/ç’°å¢ƒæª¢æŸ¥ï¼‰ï¼š

```powershell
python scripts/start_server.py
```

ä½è¨˜æ†¶é«” / è‡ªå‹•é‡åŒ–ï¼š

```powershell
powershell -ExecutionPolicy Bypass -File scripts/start_optimized.ps1
```
æˆ–ï¼ˆLinux / macOS / WSLï¼‰

```bash
bash scripts/start_optimized.sh
```

å¥åº·æª¢æŸ¥ï¼š`GET http://127.0.0.1:8001/health`

ï¼ˆå¯é¸ï¼‰æœªä¾†å¯æ–°å¢ console entry pointï¼š

```toml
[project.scripts]
agent-slm-server = "scripts.start_server:main"
```
å®‰è£å¾Œå¯ç›´æ¥åŸ·è¡Œï¼š`agent-slm-server`

 
 
## å¿«é€Ÿ API æ¸¬è©¦ï¼ˆChatï¼‰

```powershell
Invoke-RestMethod -Uri http://127.0.0.1:8001/v1/chat/completions -Method Post -Body (@{
  model = 'qwen2.5-0.5b-instruct'
  messages = @(@{role='user'; content='èªªä¸€å¥æ—©å®‰'})
} | ConvertTo-Json -Depth 5) -ContentType 'application/json'
```
> æ›´å¤šå®Œæ•´ JSON ç¯„ä¾‹èˆ‡å·¥å…·å‘¼å« / embeddings / metricsï¼šè¦‹ `USAGE_GUIDE.md`

 
## ç›®éŒ„ & æ–‡æª”å°è¦½

- é€²éšæŒ‡å—ï¼š`USAGE_GUIDE.md`
- è¨˜æ†¶é«” / é‡åŒ–ï¼š`docs/memory-optimization-guide.md`
- æ¸¬è©¦ï¼š`tests/`ï¼ˆå¯åŸ·è¡Œ `pytest -q`ï¼‰
- å•Ÿå‹•è…³æœ¬ï¼š`scripts/`
- æ¨¡å‹è³‡æºï¼š`models/`

## æ¸¬è©¦

```powershell
pytest -q            # å–®å…ƒ/æ•´åˆæ¸¬è©¦
```

> å¯æ“´å……æ›´å¤š API è¦†è“‹èˆ‡æ€§èƒ½åŸºæº–æ¸¬è©¦ï¼Œè©³è¦‹ `USAGE_GUIDE.md`ã€‚

 
## Metrics & ç›£æ§

æ”¯æŒ Prometheus `/metrics`ï¼›è©³ç´°æŒ‡æ¨™åç¨±èˆ‡è§£è®€ï¼šè¦‹ `USAGE_GUIDE.md`ã€‚

 
## ç’°å¢ƒè®Šæ•¸

å¸¸ç”¨ï¼šMODEL_QUANTIZATION / LOW_CPU_MEM_USAGE / MODEL_UNLOAD_ENABLED / MAX_INPUT_TOKENS ...

å®Œæ•´åˆ—è¡¨èˆ‡å»ºè­°å€¼ï¼š`USAGE_GUIDE.md` + `docs/memory-optimization-guide.md`ã€‚

 
## é€²éšä¸»é¡Œ
è¼¸å…¥æˆªæ–·ç­–ç•¥ / ä¸²æµ SSE / Stop Tokens / æ€§èƒ½èª¿å„ªï¼šè¦‹ `USAGE_GUIDE.md`ã€‚

## é–‹ç™¼å¿«é€Ÿæ¸¬è©¦

```powershell
python qwenchat.py             # å°è©±æ¸¬è©¦
python scripts/start_server.py # å•Ÿå‹•ï¼ˆå«æª¢æŸ¥ï¼‰
pytest -q                      # æ¸¬è©¦
```

## é·ç§»èªªæ˜ (Rename Migration)

å°ˆæ¡ˆå·²å¾ `y26hkx-llm-server` / `y26hkx_server` é‡å‘½åç‚º `agent_slm_server`ã€‚

### è®Šæ›´é‡é»

| é …ç›® | èˆŠ | æ–° |
|------|----|----|
| å¥—ä»¶åç¨± | y26hkx_server | agent_slm_server |
| ç™¼ä½ˆ distribution | y26hkx-llm-server | agent_slm_server |
| uvicorn å•Ÿå‹• | uvicorn y26hkx_server.slm_server:app | uvicorn agent_slm_server.slm_server:app |
| Metrics å‰ç¶´ | y26hkx_ | agent_slm_ |

### å‡ç´šæ­¥é©Ÿ

```powershell
pip uninstall -y y26hkx-llm-server  # è‹¥å­˜åœ¨
pip install -e .[dev]
uvicorn agent_slm_server.slm_server:app --host 127.0.0.1 --port 8001 --reload
```

### è‡ªè¨‚è…³æœ¬æ›´æ–°

è«‹æœå°‹ `y26hkx_server` ä¸¦æ›¿æ›ç‚º `agent_slm_server`ã€‚

### å…¼å®¹æ€§æç¤º

ä¸å†ä¿ç•™èˆŠæ¨¡çµ„åˆ¥åï¼›è‹¥ä»æœ‰æ­·å²ç¨‹å¼ä¾è³´è«‹åŒæ­¥æ›´æ–°ã€‚

## Roadmap

- [ ] SSE ä¸²æµè¼¸å‡ºï¼ˆOpenAI äº‹ä»¶æ ¼å¼ï¼‰
- [ ] æ›´ç²¾æº– token ç´¯ç©æˆªæ–·ç­–ç•¥ï¼ˆå°è©±æ»‘å‹•è¦–çª—ï¼‰
- [ ] ç’°å¢ƒè®Šæ•¸é›†ä¸­ç®¡ç†ï¼ˆé›†ä¸­ config æ¨¡çµ„ï¼‰
- [ ] Embeddingsï¼šnormalize é¸é …ã€æ‰¹æ¬¡å¤§å°æ§åˆ¶
- [ ] å·¥å…·å‘¼å«åœ¨ä¸²æµæ¨¡å¼ä¸‹çš„é€æ­¥è¼¸å‡º
- [ ] console entry point (`agent-slm-server`)
- [ ] å®Œæ•´ API æ–‡ä»¶è‡ªå‹•ç”Ÿæˆ (mkdocstrings)

## æˆæ¬Š

åƒ…ä¾›æœ¬åœ°ç ”ç©¶æ¸¬è©¦ï¼Œè«‹éµå¾ªå„æ¨¡å‹åŸå§‹æˆæ¬Šï¼ˆQwen / sentence-transformersï¼‰ã€‚

---
è‹¥éœ€æ–°å¢åŠŸèƒ½æˆ–å„ªåŒ–ï¼Œè«‹åœ¨è­°é¡Œä¸­æå‡ºæˆ–ç›´æ¥ä¿®æ”¹å¾Œæäº¤ PRã€‚
